# GSH_UPDATE_PROMPT gets called each time before gsh renders the prompt
# It should update the value of the $GSH_PROMPT environment variable
function GSH_UPDATE_PROMPT() {
  GSH_PROMPT="gsh> "
}

# The value of GSH_PROMPT is what gets rendered as the prompt
GSH_PROMPT="gsh> "

# The minimum log level to log.
# Can be debug, info, warn, error, panic, fatal
GSH_LOG_LEVEL="info"

# Whether gsh should remove existing content in the log file when it starts
GSH_CLEAN_LOG_FILE=0

# Minimum number of lines the shell prompt would occupy
GSH_MINIMUM_HEIGHT=8

# -------- Large Language Model Configuration --------
# - gsh invokes Large Language Models through OpenAI-compatible API
# - You can choose to use Ollama which runs LLM on your local machine
# - You can also use OpenAI or OpenRouter which runs LLM as a cloud service
# - Read the corresponding documentation of the model provider for config values below

# The "fast" model is used for auto suggestion.
# By default gsh uses qwen2.5 through Ollama as the fast model.
GSH_FAST_MODEL_API_KEY=ollama
GSH_FAST_MODEL_BASE_URL=http://localhost:11434/v1/
GSH_FAST_MODEL_ID=qwen2.5
GSH_FAST_MODEL_TEMPERATURE=0.1
GSH_FAST_MODEL_HEADERS='{}'

# The "slow" model is used for chat and agentic operations.
# By default gsh uses qwen2.5:32b through Ollama as the slow model.
GSH_SLOW_MODEL_API_KEY=ollama
GSH_SLOW_MODEL_BASE_URL=http://localhost:11434/v1/
GSH_SLOW_MODEL_ID=qwen2.5:32b
GSH_SLOW_MODEL_TEMPERATURE=0.1
GSH_SLOW_MODEL_HEADERS='{}'

# -------- RAG Configuration --------
# gsh uses Retrieval Augmented Generation (RAG) to get context from the environment and help give accurate results.
#
# Available context types include: 
# - system_info: current OS and architecture
# - working_directory: path of the current working directory
# - git_status: output from `git status`
# - history_concise: a concise version of command history
# - history_verbose: a verbose version of command history
#
# Retrieving more context will generally improve output quality at the cost of using more tokens and increased latency.

# A list of context to send to LLM along with agent chat messages.
GSH_CONTEXT_TYPES_FOR_AGENT=system_info,working_directory,git_status,history_verbose

# A list of context to send to LLM when predicting command with a partial prefix already entered by user
GSH_CONTEXT_TYPES_FOR_PREDICTION_WITH_PREFIX=system_info,working_directory,git_status,history_concise

# A list of context to send to LLM when predicting command with no prefix entered by user yet
GSH_CONTEXT_TYPES_FOR_PREDICTION_WITHOUT_PREFIX=system_info,working_directory,git_status,history_verbose

# A list of context to send to LLM when explaining command
GSH_CONTEXT_TYPES_FOR_EXPLANATION=system_info,working_directory

# How many recent commands to use in concise version of commmand history
GSH_CONTEXT_NUM_HISTORY_CONCISE=30

# How many recent commands to use in verbose version of commmand history
GSH_CONTEXT_NUM_HISTORY_VERBOSE=30

# -------- Agent Configuration --------
# Options below control behaviors of the chat agent.

# Size of the agent chat context window in LLM tokens.
# When the chat session exceeds this limit, only the most recent messages that 
# can fit in the window are kept.
GSH_AGENT_CONTEXT_WINDOW_TOKENS=32768

# A JSON array of regex patterns for bash commands that should be considered pre-approved
# Pre-approved commands will be executed without asking for confirmation
GSH_AGENT_APPROVED_BASH_COMMAND_REGEX='[
  "^ls\\s.*$",
  "^git\\s+status.*$",
  "^pwd$",
  "^echo\\s.*$",
  "^cat\\s.*$"
]'
