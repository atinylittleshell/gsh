# GSH_UPDATE_PROMPT gets called each time before gsh renders the prompt
# It should update the value of the $GSH_PROMPT environment variable
function GSH_UPDATE_PROMPT() {
  GSH_PROMPT="gsh> "
}

# The value of GSH_PROMPT is what gets rendered as the prompt
GSH_PROMPT="gsh> "

# The minimum log level to log.
# Can be debug, info, warn, error, panic, fatal
GSH_LOG_LEVEL="info"

# Whether gsh should remove existing content in the log file when it starts
GSH_CLEAN_LOG_FILE=0

# -------- Large Language Model Configuration --------
# - gsh invokes Large Language Models through OpenAI-compatible API
# - You can choose to use Ollama which runs LLM on your local machine
# - You can also use OpenAI or OpenRouter which runs LLM as a cloud service
# - Read the corresponding documentation of the model provider for config values below

# The "fast" model is used for auto suggestion.
GSH_FAST_MODEL_API_KEY=ollama
GSH_FAST_MODEL_BASE_URL=http://localhost:11434/v1/
GSH_FAST_MODEL_ID=qwen2.5
GSH_FAST_MODEL_TEMPERATURE=0.1
GSH_FAST_MODEL_HEADERS='{}'

# The "slow" model is used for chat and agentic operations.
GSH_SLOW_MODEL_API_KEY=ollama
GSH_SLOW_MODEL_BASE_URL=http://localhost:11434/v1/
GSH_SLOW_MODEL_ID=qwen2.5:32b
GSH_SLOW_MODEL_TEMPERATURE=0.1
GSH_SLOW_MODEL_HEADERS='{}'

# How many past commands to use as context for prediction and agent chat
GSH_PREDICTION_HISTORY_CONTEXT_LIMIT=30

# Size of the agent chat context window in LLM tokens.
# When the chat session exceeds this limit, only the most recent messages that 
# can fit in the window are kept.
GSH_AGENT_CONTEXT_WINDOW_TOKENS=32768
